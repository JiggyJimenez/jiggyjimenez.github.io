{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6846eec3-017f-4ef4-a1d5-4a6e591ab430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T16:59:52.254459Z",
     "iopub.status.busy": "2023-07-27T16:59:52.253921Z",
     "iopub.status.idle": "2023-07-27T16:59:52.262369Z",
     "shell.execute_reply": "2023-07-27T16:59:52.260582Z",
     "shell.execute_reply.started": "2023-07-27T16:59:52.254412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from PIL import Image\n",
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models.segmentation as segmentation\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3306a48-44dc-4c8d-9d78-4e0a488d1113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T17:30:53.463339Z",
     "iopub.status.busy": "2023-07-27T17:30:53.460840Z",
     "iopub.status.idle": "2023-07-27T17:30:53.492651Z",
     "shell.execute_reply": "2023-07-27T17:30:53.491078Z",
     "shell.execute_reply.started": "2023-07-27T17:30:53.463298Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_and_remove_background(image_paths, output_size_inches=1, dpi=300, surroundings_factor=0.7, output_path_template=\"face_{}_{}.png\", output_folder=\"output_images\"):\n",
    "    # Define the U-Net model with ResNet50 backbone\n",
    "    model = torch.hub.load(\"pytorch/vision\", \"deeplabv3_resnet50\", pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directory if it does not exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    output_size_pixels = output_size_inches * dpi\n",
    "\n",
    "    for image_index, image_path in enumerate(image_paths):\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Convert the image to NumPy array\n",
    "        img_array = np.array(image)\n",
    "\n",
    "        # Create an MTCNN detector\n",
    "        detector = MTCNN()\n",
    "\n",
    "        # Detect faces in the image\n",
    "        face_locations = detector.detect_faces(img_array)\n",
    "\n",
    "        # Check if faces are found\n",
    "        if len(face_locations) == 0:\n",
    "            print(f\"No faces found in image {image_index + 1}.\")\n",
    "            continue\n",
    "\n",
    "        # Crop and save the faces with specified surroundings_factor\n",
    "        for i, face_location in enumerate(face_locations):\n",
    "            x, y, width, height = face_location['box']\n",
    "\n",
    "            # Calculate the new dimensions for cropping based on surroundings_factor\n",
    "            new_width = int(width * (1.0 + surroundings_factor))\n",
    "            new_height = int(height * (1.0 + surroundings_factor))\n",
    "\n",
    "            # Find the maximum dimension (width or height) of the detected face\n",
    "            max_dimension = max(new_width, new_height)\n",
    "\n",
    "            # Calculate the top-left and bottom-right coordinates for the square crop\n",
    "            new_x1 = max(0, x + (width - max_dimension) // 2)\n",
    "            new_y1 = max(0, y + (height - max_dimension) // 2)\n",
    "            new_x2 = min(img_array.shape[1], x + (width + max_dimension) // 2)\n",
    "            new_y2 = min(img_array.shape[0], y + (height + max_dimension) // 2)\n",
    "\n",
    "            # Crop the image with the new dimensions\n",
    "            face = img_array[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "            # Resize the face to the desired output size\n",
    "            face_img = Image.fromarray(face)\n",
    "            aspect_ratio = face_img.width / face_img.height\n",
    "            if aspect_ratio > 1:  # Image is wider than it is tall\n",
    "                new_height = output_size_pixels\n",
    "                new_width = int(new_height * aspect_ratio)\n",
    "            else:  # Image is taller than it is wide, or is square\n",
    "                new_width = output_size_pixels\n",
    "                new_height = int(new_width / aspect_ratio)\n",
    "            face_img = face_img.resize((new_width, new_height))\n",
    "\n",
    "            output_path = os.path.join(output_folder, output_path_template.format(image_index + 1, i + 1))\n",
    "            face_img.save(output_path)\n",
    "            print(f\"Face {i+1} in image {image_index + 1} saved at {output_path}.\")\n",
    "\n",
    "            # Remove background from the saved face\n",
    "            # Load the input image\n",
    "            face_image = Image.open(output_path)\n",
    "\n",
    "            # Preprocess the image for the model\n",
    "            preprocess = transforms.Compose([\n",
    "                transforms.Resize((512, 512)),  # Adjust the size to fit the model input size\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            input_tensor = preprocess(face_image)\n",
    "            input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "            # Run the image through the model\n",
    "            with torch.no_grad():\n",
    "                output = model(input_batch)[\"out\"][0]\n",
    "\n",
    "            # Convert the output to a binary mask\n",
    "            mask = (output.argmax(0) == 15).float()\n",
    "\n",
    "            # Resize the mask to match the size of the original image\n",
    "            resize_transform = transforms.Resize(face_image.size, interpolation=Image.NEAREST)\n",
    "            mask_resized = resize_transform(mask.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "            # Create a new RGBA image with the original image and the transparent background\n",
    "            rgba_image = Image.new(\"RGBA\", face_image.size)\n",
    "            rgba_image.paste(face_image, (0, 0))\n",
    "\n",
    "            # Convert the mask to a NumPy array and apply as the alpha channel\n",
    "            mask_np = (mask_resized * 255).byte().numpy()\n",
    "            mask_image = Image.fromarray(mask_np, mode=\"L\")\n",
    "            rgba_image.putalpha(mask_image)\n",
    "\n",
    "            # Save the result\n",
    "            rgba_image.save(output_path)\n",
    "\n",
    "    # Zip the output folder if there are multiple images\n",
    "    if len(image_paths) > 1:\n",
    "        with zipfile.ZipFile(\"output_faces.zip\", \"w\") as zipf:\n",
    "            for root, _, files in os.walk(output_folder):\n",
    "                for file in files:\n",
    "                    zipf.write(os.path.join(root, file), os.path.basename(file))\n",
    "        print(\"Output images have been zipped to 'output_faces.zip'.\")\n",
    "\n",
    "        # Delete unzipped files after zipping\n",
    "        for root, _, files in os.walk(output_folder):\n",
    "            for file in files:\n",
    "                os.remove(os.path.join(root, file))\n",
    "        print(\"Unzipped output images have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8247082f-b22a-4dc0-a554-b95e283f19ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T17:30:54.456352Z",
     "iopub.status.busy": "2023-07-27T17:30:54.455788Z",
     "iopub.status.idle": "2023-07-27T17:31:33.957428Z",
     "shell.execute_reply": "2023-07-27T17:31:33.955450Z",
     "shell.execute_reply.started": "2023-07-27T17:30:54.456301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/msds2023/jrjimenez/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "Face 1 in image 1 saved at output_images/face_1_1.png.\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "Face 1 in image 2 saved at output_images/face_2_1.png.\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "Face 1 in image 3 saved at output_images/face_3_1.png.\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Face 1 in image 4 saved at output_images/face_4_1.png.\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "Face 1 in image 5 saved at output_images/face_5_1.png.\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "Face 1 in image 6 saved at output_images/face_6_1.png.\n",
      "Output images have been zipped to 'output_faces.zip'.\n",
      "Unzipped output images have been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Example usage with a list of image paths\n",
    "image_paths = [\"Photos/photo_6204012424814769816_y.jpg\", \"Photos/photo_6204012424814769817_y.jpg\", \"Photos/photo_6204012424814769818_y.jpg\",\n",
    "               \"Photos/photo_6204012424814769819_y.jpg\", \"Photos/photo_6204012424814769820_y.jpg\", \"Photos/photo_6204012424814769821_y.jpg\"]\n",
    "process_and_remove_background(image_paths, surroundings_factor=0.7, output_path_template=\"face_{}_{}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b637dc-7b4b-4be6-925a-bd7ec9541208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
